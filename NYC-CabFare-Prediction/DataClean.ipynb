{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import needed libraries\n",
    "import pandas as pd # CSV file I/O (e.g. pd.read_csv)\n",
    "import os # reading the input files we have access to\n",
    "import numpy as np\n",
    "import datetime as dt # handle datetime\n",
    "import matplotlib,math\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "import seaborn as sb\n",
    "import sklearn\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.preprocessing import scale \n",
    "from collections import Counter \n",
    "from matplotlib.collections import LineCollection\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import PassiveAggressiveRegressor\n",
    "from sklearn.datasets import make_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the first few lines of the training set, prepare testing set for submission, and validation set for make shift test  \n",
    "# Set the number of rows for training here:\n",
    "training_size = 5000000\n",
    "\n",
    "train_df =  pd.read_csv('../all/train.csv',nrows = training_size) #100,000 # 5000000 data points\n",
    "test_df = pd.read_csv('../all/test.csv')\n",
    "validation_df = pd.read_csv('../all/train.csv',skiprows=training_size,nrows=len(test_df))\n",
    "validation_df.columns = [\"key\",\"fare_amount\",\"pickup_datetime\",\"pickup_longitude\",\"pickup_latitude\",\"dropoff_longitude\",\"dropoff_latitude\",\"passenger_count\"]\n",
    "\n",
    "# change 'pickup_datetime' to pandas DF for future processing\n",
    "train_df['pickup_datetime'] = pd.to_datetime(train_df.pickup_datetime)\n",
    "test_df['pickup_datetime'] = pd.to_datetime(test_df.pickup_datetime)\n",
    "validation_df['pickup_datetime'] = pd.to_datetime(validation_df.pickup_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new features from the provided dataframe\n",
    "def feature_generator(df):\n",
    "    df['year'] = df.pickup_datetime.dt.year\n",
    "    df['month'] = df.pickup_datetime.dt.month\n",
    "    df['day'] = df.pickup_datetime.dt.day\n",
    "    df['hour'] = df.pickup_datetime.dt.hour\n",
    "    df['minute'] = df.pickup_datetime.dt.minute\n",
    "    df['week'] = df.pickup_datetime.dt.week\n",
    "    df['weekofyear'] = df.pickup_datetime.dt.weekofyear\n",
    "    df['dayofyear'] = df.pickup_datetime.dt.dayofyear\n",
    "    df['quarter'] = df.pickup_datetime.dt.quarter\n",
    "    df['abs_diff_longitude'] = (df.dropoff_longitude - df.pickup_longitude).abs()\n",
    "    df['abs_diff_latitude'] = (df.dropoff_latitude - df.pickup_latitude).abs()\n",
    "    df['initial_fair'] = 3.5 # add a column for initial_fair\n",
    "\n",
    "# Update the train dataframe with the new features\n",
    "feature_generator(train_df)\n",
    "\n",
    "# create new features from the provided dataframe\n",
    "def normalize_features(df):\n",
    "    df['month'] = (df['month'] - df['month'].mean()) / (df['month'].max() - df['month'].min())\n",
    "    df['day'] = (df['day'] - df['day'].mean()) / (df['day'].max() - df['day'].min())\n",
    "    df['hour'] = (df['hour'] - df['hour'].mean()) / (df['hour'].max() - df['hour'].min())\n",
    "    df['minute'] = (df['minute'] - df['minute'].mean()) / (df['minute'].max() - df['minute'].min())  \n",
    "    df['abs_diff_longitude'] = (df['abs_diff_longitude'] - df['abs_diff_longitude'].mean()) / (df['abs_diff_longitude'].max() - df['abs_diff_longitude'].min())\n",
    "    df['abs_diff_latitude'] = (df['abs_diff_latitude'] - df['abs_diff_latitude'].mean()) / (df['abs_diff_latitude'].max() - df['abs_diff_latitude'].min())\n",
    "\n",
    "# Update the train dataframe with the new features\n",
    "feature_generator(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### clean the data: ###\n",
    "train_df.isnull().sum()\n",
    "train_df.dropna(axis=0, how='any', inplace=True)\n",
    "train_df.passenger_count.value_counts()\n",
    "train_df = train_df[(train_df.passenger_count>0)&(train_df.passenger_count<10)]\n",
    "train_df = train_df[(train_df.abs_diff_longitude < 5.0) & (train_df.abs_diff_latitude < 5.0)]\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Outlier elimination: \n",
    "\n",
    "outliers = []\n",
    "# For each feature find the data points with extreme high or low values\n",
    "for feature in ['abs_diff_longitude','abs_diff_latitude']:\n",
    "    Q1 = np.percentile(train_df[feature],25,axis=0)\n",
    "    Q3 = np.percentile(train_df[feature],75,axis=0)\n",
    "    step = 10*(Q3-Q1)\n",
    "    feature_outlier = train_df[~((train_df[feature] >= Q1 - step) & (train_df[feature] <= Q3 + step))]\n",
    "    outliers += feature_outlier.index.tolist()\n",
    "\n",
    "\n",
    "# Drop outliers\n",
    "train_df = train_df.drop(train_df.index[outliers]).reset_index(drop = True)\n",
    "\n",
    "# drop unwanted features now after pre-processing: \n",
    "# drop the pickup_datetime as it is no longer needed\n",
    "train_df=train_df.drop(['pickup_datetime'],axis=1)\n",
    "# drop the key as it is no longer needed\n",
    "train_df=train_df.drop(['key'],axis=1)\n",
    "\n",
    "# normalize features\n",
    "normalize_features(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create predictors feature set and the target outcome:\n",
    "RegPredictors = train_df.iloc[:,1:18].values # Predictors are simply the features\n",
    "RegTarget = train_df.iloc[:,0].values # Target is the fare amount\n",
    "\n",
    "# create testing features and load test_RegPredictors\n",
    "feature_generator(test_df)\n",
    "normalize_features(test_df)\n",
    "test_RegPredictors = test_df.iloc[:,2:19].values\n",
    "\n",
    "# prepare validation set and make it look like and load test_RegPredictors\n",
    "# save fare amount for validation\n",
    "validation_fare_amount = validation_df.iloc[:,1].values\n",
    "validation_df=validation_df.drop(['fare_amount'],axis=1)\n",
    "feature_generator(validation_df)\n",
    "normalize_features(validation_df)\n",
    "validation_RegPredictors = test_df.iloc[:,2:19].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # test regression models here: \n",
    "\n",
    "# SVR_Reg = SVR(C=1.0, epsilon=0.2)\n",
    "# SVR_Reg.fit(RegPredictors, RegTarget)\n",
    "# RegTarget_predictions = SVR_Reg.predict(test_RegPredictors).round(decimals = 2)\n",
    "\n",
    "RF_Reg = RandomForestRegressor(n_jobs=-1)\n",
    "RF_Reg.fit(RegPredictors, RegTarget)\n",
    "RegTarget_predictions = RF_Reg.predict(test_RegPredictors)\n",
    "m = RF_Reg\n",
    "\n",
    "# regr = PassiveAggressiveRegressor()\n",
    "# regr.fit(RegPredictors, RegTarget)\n",
    "# PassiveAggressiveRegressor(C=1.0, average=False, epsilon=0.1,fit_intercept=True, loss='epsilon_insensitive',max_iter=10000, n_iter=None, random_state=0, shuffle=True,tol=None, verbose=0, warm_start=True)\n",
    "# RegTarget_predictions = regr.predict(test_RegPredictors)\n",
    "# m = regr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.8014543651386732, 10.76678118192561, 0.960093678798785, -0.1792379658135923]\n"
     ]
    }
   ],
   "source": [
    "# define RMSE for testing\n",
    "def rmse(x,y): return math.sqrt(((x-y)**2).mean())\n",
    "\n",
    "def print_score(m):\n",
    "    res = [rmse(m.predict(RegPredictors), RegTarget), rmse(m.predict(validation_RegPredictors), validation_fare_amount),\n",
    "                m.score(RegPredictors, RegTarget), m.score(validation_RegPredictors, validation_fare_amount)]\n",
    "    print(res)\n",
    "\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [4.619374837151818, 13.321280085846855, 0.737601364373413, -0.8051845116119909] 3.71\n",
    "# [1.8171518549456913, 13.21201823904891, 0.9603583437562129, -0.934987861504124] 3.85\n",
    "# [1.9403859091404618, 13.084358576159007, 0.9547992571764596, -0.8977753035973798] 4.084\n",
    "# [9.49306218252571, 9.844027629785927, -0.08188724075898235, -0.07420102842762133] 3.92\n",
    "# [10.279379265104573, 11.238026320440355, -0.29935835655106113, -0.2847237986374469] 9.80594\n",
    "# [9.785665834727306, 10.416795439651933, -0.14336128593079978, -0.15707856825155386] 0.08105\n",
    "# [1.8014543651386732, 10.76678118192561, 0.960093678798785, -0.1792379658135923] 6.89398"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.DS_Store', 'test.csv', 'submission.csv', 'code.py', 'aLogictest_RFsubmission.csv', '__pycache__', 'test.py', 'GCP-Coupons-Instructions.rtf', 'test_RFsubmission.csv', 'train.csv', '.ipynb_checkpoints', 'Taxi.ipynb', 'sample_submission.csv', 'DataClean.ipynb']\n"
     ]
    }
   ],
   "source": [
    "# Write the predictions to a CSV file which we can submit to the competition.\n",
    "submission = pd.DataFrame(\n",
    "    {'key': test_df.key, 'fare_amount': RegTarget_predictions},\n",
    "    columns = ['key', 'fare_amount'])\n",
    "submission.to_csv('aLogictest_RFsubmission.csv', index = False)\n",
    "\n",
    "print(os.listdir('.'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
